{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6efedddc-0193-4568-8ca0-31f3e0df2b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "603a84a7-a403-42c6-ab8a-1e334e07f68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classification_report_per_dataset(dataset, model_columns, true_label_column='label'):\n",
    "    \n",
    "    detailed_results = []\n",
    "    error_counts = {}  \n",
    "    \n",
    "    true_labels = dataset[true_label_column]\n",
    "\n",
    "    for model_col in model_columns:\n",
    "\n",
    "        predictions = dataset[model_col]\n",
    "        \n",
    "        # Count the 'error' occurrences\n",
    "        error_count = (predictions == 'error').sum() + (predictions == '').sum()\n",
    "        error_counts[model_col] = error_count  \n",
    "        \n",
    "        # Replace 'error' with a special class for classification report\n",
    "        predictions = predictions.replace('error', 'error_class')\n",
    "        predictions = predictions.replace('', 'error_class')\n",
    "        report = classification_report(true_labels, predictions, output_dict=True, zero_division=0)\n",
    "        \n",
    "        # Add detailed results\n",
    "        for label, metrics in report.items():\n",
    "            if isinstance(metrics, dict):  \n",
    "                detailed_results.append({\n",
    "                    'Model': model_col,\n",
    "                    'Class': label,\n",
    "                    'Precision': metrics.get('precision'),\n",
    "                    'Recall': metrics.get('recall'),\n",
    "                    'F1-Score': metrics.get('f1-score'),\n",
    "                    'Support': metrics.get('support'),\n",
    "                })\n",
    "\n",
    "\n",
    "    detailed_df = pd.DataFrame(detailed_results)\n",
    "\n",
    "    # Update support for 'error' class using the error_counts dictionary\n",
    "    for model, error_count in error_counts.items():\n",
    "        detailed_df.loc[(detailed_df['Model'] == model) & (detailed_df['Class'] == 'error_class'), 'Support'] = error_count\n",
    "\n",
    "    return detailed_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "63f1c7fc-7376-4341-9905-b4a89991cfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_name = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8c5cd12b-6d80-46b8-8f67-65b91ede2452",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_feather(f'datasets/{ds_name}.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a173fef-0387-4e6f-bf10-b01e84d1aec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['Llama-3.1-8B-Instruct','Llama-3.1-70B-Instruct', 'Ministral-8B-Instruct-2410', 'gemma-2-9b-it',\n",
    "       'SauerkrautLM-gemma-2-9b-it','Teuken-7B-instruct-research-v0.4','Llama-3.3-70B-Instruct','Llama-3.1-SauerkrautLM-8b-Instruct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7e7353ed-4de0-427b-b53d-346e2011000b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_repo= generate_classification_report_per_dataset(df,models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2bcf5e71-aea3-4994-af5e-391a7419bc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_repo.to_csv(f'results/{ds_name}_classrepo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "59af450c-dcfb-4533-8194-925d58d9cd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Initialize lists to store results\n",
    "error_per_model_per_dataset = []\n",
    "error_per_model_total = {}\n",
    "\n",
    "# Iterate through all CSV files in the folder\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        \n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        error_df = df[df['Class'] == 'error_class']\n",
    "        \n",
    "        # Sum errors per Model in the dataset\n",
    "        for model, support_sum in error_df.groupby('Model')['Support'].sum().items():\n",
    "            error_per_model_per_dataset.append({\n",
    "                'Dataset': file, \n",
    "                'Model': model, \n",
    "                'Total Errors': support_sum\n",
    "            })\n",
    "            \n",
    "            # Aggregate total errors per Model across all datasets\n",
    "            if model in error_per_model_total:\n",
    "                error_per_model_total[model] += support_sum\n",
    "            else:\n",
    "                error_per_model_total[model] = support_sum\n",
    "\n",
    "df_per_dataset = pd.DataFrame(error_per_model_per_dataset)\n",
    "df_total_errors = pd.DataFrame(error_per_model_total.items(), columns=['Model', 'Total Errors Across Datasets'])\n",
    "\n",
    "df_per_dataset.to_csv('error_ds.csv',index=False)\n",
    "df_total_errors.to_csv('error_sum.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SentiLLM",
   "language": "python",
   "name": "sentillm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
